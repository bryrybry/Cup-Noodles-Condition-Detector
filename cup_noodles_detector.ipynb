{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ff643c",
   "metadata": {},
   "source": [
    "# 1. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a27776",
   "metadata": {},
   "source": [
    "### ‚öñÔ∏è Check Class Balance\n",
    "**Displays the number and percentage of images per class.**\n",
    "\n",
    "*(Ensuring that each class has a similar amount of images will improve the model)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9f1fd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts: {'Finished': 438, 'Opened': 287, 'Sealed': 314}\n",
      "Ratios: {'Finished': '42.2%', 'Opened': '27.6%', 'Sealed': '30.2%'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def check_balance(data_dir='./data'):\n",
    "    counts = {}\n",
    "    folders = []  \n",
    "    \n",
    "    for class_dir in os.listdir(data_dir):\n",
    "        class_path = os.path.join(data_dir, class_dir)\n",
    "        if os.path.isdir(class_path):\n",
    "            folders.append(class_dir)\n",
    "            counts[class_dir] = len(os.listdir(class_path))\n",
    "    \n",
    "    print(\"Counts:\", counts)\n",
    "    print(\"Ratios:\", {k: f\"{(counts[k]/sum(counts.values())*100):.1f}%\" for k in folders})\n",
    "\n",
    "check_balance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4f0b6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMG_SIZE\n",
      "BATCH_SIZE\n",
      "EPOCHS\n",
      "NUM_CLASSES\n",
      "DATA_DIR\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "CONFIG = {\n",
    "    'IMG_SIZE': (75, 75),\n",
    "    'BATCH_SIZE': 32, \n",
    "    'EPOCHS': 20,\n",
    "    'NUM_CLASSES': 3,\n",
    "    'DATA_DIR': './data'\n",
    "}\n",
    "\n",
    "for config in CONFIG:\n",
    "    print(config)\n",
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45c4908a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking dataset...\n",
      "Dataset Balance:\n",
      "{'Finished': 438, 'Opened': 287, 'Sealed': 314}\n",
      "Ratios: {'Finished': '42.2%', 'Opened': '27.6%', 'Sealed': '30.2%'}\n",
      "\n",
      "üìä Loading data...\n",
      "Found 833 images belonging to 3 classes.\n",
      "Found 911 images belonging to 3 classes.\n",
      "Found 128 images belonging to 3 classes.\n",
      "\n",
      "üèóÔ∏è Building model...\n",
      "\n",
      "‚öñÔ∏è Computing class weights...\n",
      "Class weights: {0: np.float64(0.7910731244064577), 1: np.float64(1.2072463768115942), 2: np.float64(1.1018518518518519)}\n",
      "\n",
      "üöÄ Training...\n",
      "Epoch 1/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 4s/step - accuracy: 0.5990 - loss: 1.2552 - val_accuracy: 0.8101 - val_loss: 0.4866 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 4s/step - accuracy: 0.7239 - loss: 0.8375 - val_accuracy: 0.7475 - val_loss: 0.5711 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 5s/step - accuracy: 0.7743 - loss: 0.6203 - val_accuracy: 0.8255 - val_loss: 0.4284 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 4s/step - accuracy: 0.7911 - loss: 0.5140 - val_accuracy: 0.8441 - val_loss: 0.3921 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 5s/step - accuracy: 0.7959 - loss: 0.5301 - val_accuracy: 0.8661 - val_loss: 0.3444 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 4s/step - accuracy: 0.8031 - loss: 0.4902 - val_accuracy: 0.8397 - val_loss: 0.3875 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 4s/step - accuracy: 0.7995 - loss: 0.4961 - val_accuracy: 0.8617 - val_loss: 0.3491 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 4s/step - accuracy: 0.8043 - loss: 0.5063 - val_accuracy: 0.8825 - val_loss: 0.2885 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 5s/step - accuracy: 0.8103 - loss: 0.4792 - val_accuracy: 0.8793 - val_loss: 0.3098 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 5s/step - accuracy: 0.8331 - loss: 0.4215 - val_accuracy: 0.8716 - val_loss: 0.3220 - learning_rate: 0.0010\n",
      "Epoch 11/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 5s/step - accuracy: 0.8319 - loss: 0.4524 - val_accuracy: 0.9089 - val_loss: 0.2436 - learning_rate: 0.0010\n",
      "Epoch 12/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 5s/step - accuracy: 0.8415 - loss: 0.4058 - val_accuracy: 0.9122 - val_loss: 0.2520 - learning_rate: 0.0010\n",
      "Epoch 13/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 5s/step - accuracy: 0.8415 - loss: 0.3957 - val_accuracy: 0.8979 - val_loss: 0.2395 - learning_rate: 0.0010\n",
      "Epoch 14/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 4s/step - accuracy: 0.8535 - loss: 0.3749 - val_accuracy: 0.8683 - val_loss: 0.3280 - learning_rate: 0.0010\n",
      "Epoch 15/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 4s/step - accuracy: 0.8487 - loss: 0.4349 - val_accuracy: 0.9199 - val_loss: 0.2148 - learning_rate: 0.0010\n",
      "Epoch 16/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 4s/step - accuracy: 0.8355 - loss: 0.4199 - val_accuracy: 0.9254 - val_loss: 0.2231 - learning_rate: 0.0010\n",
      "Epoch 17/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 4s/step - accuracy: 0.8319 - loss: 0.4158 - val_accuracy: 0.9210 - val_loss: 0.2097 - learning_rate: 0.0010\n",
      "Epoch 18/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 4s/step - accuracy: 0.8727 - loss: 0.3586 - val_accuracy: 0.9067 - val_loss: 0.2549 - learning_rate: 0.0010\n",
      "Epoch 19/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 4s/step - accuracy: 0.8295 - loss: 0.4123 - val_accuracy: 0.9111 - val_loss: 0.2624 - learning_rate: 0.0010\n",
      "Epoch 20/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 4s/step - accuracy: 0.8535 - loss: 0.3895 - val_accuracy: 0.9100 - val_loss: 0.2377 - learning_rate: 0.0010\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 394ms/step - accuracy: 0.8125 - loss: 0.4250\n",
      "\n",
      "‚úÖ Test accuracy: 0.8125\n",
      "üíæ Saved: cup_noodle_classifier_20260129_113032.keras\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_names = ['Finished', 'Opened', 'Sealed']\n",
    "\n",
    "def check_dataset_balance(data_dir):\n",
    "    \"\"\"Verify class distribution before training.\"\"\"\n",
    "    counts = {}\n",
    "    for class_dir in sorted(os.listdir(data_dir)):  # Consistent order\n",
    "        class_path = os.path.join(data_dir, class_dir)\n",
    "        if os.path.isdir(class_path):\n",
    "            counts[class_dir] = len([f for f in os.listdir(class_path) \n",
    "                                   if f.lower().endswith(('.jpg', '.png', '.jpeg'))])\n",
    "    \n",
    "    total = sum(counts.values())\n",
    "    print(\"Dataset Balance:\")\n",
    "    print(counts)\n",
    "    print(\"Ratios:\", {k: f\"{v/total*100:.1f}%\" for k,v in counts.items()})\n",
    "    return counts\n",
    "\n",
    "def get_image_generators(data_dir, img_size, batch_size):\n",
    "    \"\"\"Simplified 80/10/10 split - more reliable than validation_split trick.\"\"\"\n",
    "    seed = 42\n",
    "    \n",
    "    # Train: augmentation\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        validation_split=0.2  # 80/20 split\n",
    "    )\n",
    "    \n",
    "    train_gen = train_datagen.flow_from_directory(\n",
    "        data_dir, target_size=img_size, batch_size=batch_size,\n",
    "        class_mode='categorical', subset='training', seed=seed\n",
    "    )\n",
    "    \n",
    "    # Val/Test: no augmentation\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.125)  # 10% val, 10% test\n",
    "    \n",
    "    val_gen = val_datagen.flow_from_directory(\n",
    "        data_dir, target_size=img_size, batch_size=batch_size,\n",
    "        class_mode='categorical', subset='training', seed=seed, shuffle=False\n",
    "    )\n",
    "    \n",
    "    test_gen = val_datagen.flow_from_directory(\n",
    "        data_dir, target_size=img_size, batch_size=batch_size,\n",
    "        class_mode='categorical', subset='validation', seed=seed, shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_gen, val_gen, test_gen\n",
    "\n",
    "def build_improved_model(img_shape, num_classes):\n",
    "    \"\"\"Better architecture: GlobalAvgPool2D + more layers.\"\"\"\n",
    "    base_model = InceptionV3(input_shape=(*img_shape, 3), include_top=False, weights='imagenet')\n",
    "    \n",
    "    # Freeze base\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Better head (no Flatten = less params)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "# MAIN TRAINING PIPELINE\n",
    "print(\"üîç Checking dataset...\")\n",
    "check_dataset_balance(CONFIG['DATA_DIR'])\n",
    "\n",
    "print(\"\\nüìä Loading data...\")\n",
    "train_gen, val_gen, test_gen = get_image_generators(\n",
    "    CONFIG['DATA_DIR'], CONFIG['IMG_SIZE'], CONFIG['BATCH_SIZE']\n",
    ")\n",
    "\n",
    "print(\"\\nüèóÔ∏è Building model...\")\n",
    "model = build_improved_model(CONFIG['IMG_SIZE'], CONFIG['NUM_CLASSES'])\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "# model.summary()\n",
    "\n",
    "# Compute class weights for imbalance (52%/35%/13% ‚Üí weights adjust)\n",
    "print(\"\\n‚öñÔ∏è Computing class weights...\")\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced', classes=np.unique(train_gen.classes), y=train_gen.classes\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(\"Class weights:\", class_weight_dict)\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, restore_best_weights=True, monitor='val_accuracy'),\n",
    "    ReduceLROnPlateau(factor=0.2, patience=3, min_lr=1e-7)\n",
    "]\n",
    "\n",
    "print(\"\\nüöÄ Training...\")\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=CONFIG['EPOCHS'],\n",
    "    validation_data=val_gen,\n",
    "    class_weight=class_weight_dict,  # Handles your imbalance!\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Test accuracy:\", model.evaluate(test_gen)[1])\n",
    "\n",
    "# Save with timestamp\n",
    "import datetime\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model.save(f'./models/cup_noodle_classifier_{timestamp}.keras')\n",
    "print(f\"üíæ Saved: cup_noodle_classifier_{timestamp}.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6963c87",
   "metadata": {},
   "source": [
    "### ‚≠ê Save the Model as a .keras File (so it can be reused later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd5e50db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if not model: \n",
    "    print(\"There's no model yet!\")\n",
    "    sys.exit()  \n",
    "\n",
    "# get id for new model\n",
    "folder = \"./models/\"\n",
    "id = len([f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))])\n",
    "default_filename = f\"model{id}\"\n",
    "\n",
    "new_model_filename = input(f\"Enter filename (Default: {default_filename}): \")\n",
    "if new_model_filename.strip() == \"\":\n",
    "    new_model_filename = default_filename\n",
    "tf.keras.models.save_model(model, f'./models/{new_model_filename}.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cb305a",
   "metadata": {},
   "source": [
    "# 2. Trying the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f7e9e",
   "metadata": {},
   "source": [
    "### üîÉ Load a .keras model\n",
    "Loading a model is necessary if the above code to train a new model was not executed.\n",
    "(Skip this if model was recently trained above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a8dfae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model Loaded: cup_noodle_classifier_20260129_113032.keras\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "folder = \"./models/\"\n",
    "files = os.listdir(folder)\n",
    "\n",
    "# sort by last added time\n",
    "if not files: raise FileNotFoundError(\"No models found!\")\n",
    "files_sorted = sorted(files, key=lambda f: os.path.getmtime(os.path.join(folder, f)))\n",
    "last_added_filename = files_sorted[-1]\n",
    "\n",
    "while True:\n",
    "    input_filename = input(\n",
    "        f\"Enter model to load (Latest: {last_added_filename.replace('.keras', '')}): \"\n",
    "    ).strip()\n",
    "\n",
    "    if input_filename == \"\":\n",
    "        model_filename = last_added_filename\n",
    "        break\n",
    "\n",
    "    model_filename = f\"{input_filename.replace('.keras', '')}.keras\"\n",
    "    full_path = os.path.join(folder, model_filename)\n",
    "\n",
    "    if os.path.exists(full_path):\n",
    "        break\n",
    "    else:\n",
    "        print(\"‚ùå Model not found. Please try again.\")\n",
    "\n",
    "model = tf.keras.models.load_model(f'./models/{model_filename}')\n",
    "\n",
    "print(\"‚úÖ Model Loaded:\", model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c72f5ac",
   "metadata": {},
   "source": [
    "### Part 2A: Inference using Camera Stream üì∑ \n",
    "(Please view the cell under this one to terminate the camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6a6ca96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera Started!\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import tensorflow as tf\n",
    "\n",
    "def import_and_predict(image_data):\n",
    "    image = ImageOps.fit(image_data, (75, 75), Image.Resampling.LANCZOS)\n",
    "    image = image.convert('RGB')\n",
    "    image = np.asarray(image)\n",
    "    image = (image.astype(np.float32) / 255.0)\n",
    "    img_reshape = image[np.newaxis,...]\n",
    "    prediction = model.predict(img_reshape, verbose=0)\n",
    "    return prediction\n",
    "\n",
    "stop_flag = False\n",
    "\n",
    "def camera_loop():\n",
    "    global stop_flag\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        cap.open()\n",
    "        \n",
    "    print(\"Camera Started!\")\n",
    "\n",
    "    while not stop_flag:\n",
    "        ret, original = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image = Image.fromarray(cv2.cvtColor(original, cv2.COLOR_BGR2RGB))\n",
    "        prediction = import_and_predict(image)\n",
    "\n",
    "        map = {0: \"Finished\",\n",
    "               1: \"Opened\",\n",
    "               2: \"Sealed\"}\n",
    "\n",
    "        idx = np.argmax(prediction)\n",
    "        confidence = prediction[0][idx] \n",
    "\n",
    "        predict = f\"It is a {map[idx]}! ({confidence:.2f})\"\n",
    "\n",
    "        cv2.putText(original, predict, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "        cv2.imshow(\"Cup Noodles Detector\", original)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            stop_flag = True\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# start the camera in a separate thread\n",
    "camera_thread = threading.Thread(target=camera_loop)\n",
    "camera_thread.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0569d8e4",
   "metadata": {},
   "source": [
    "‚úã Run to terminate the camera!\n",
    "   üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87c4efcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped!\n"
     ]
    }
   ],
   "source": [
    "stop_flag = True\n",
    "camera_thread.join()\n",
    "print(\"Stopped!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f930f5cb",
   "metadata": {},
   "source": [
    "### Part 2B: Inference using Folder of Images\n",
    "For this part, we will classify your images based on uploaded images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf3257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "\n",
    "COLUMNS = 5\n",
    "\n",
    "def select_images_with_tkinter():\n",
    "    \"\"\"\n",
    "    Opens tkinter file dialog to select multiple image files.\n",
    "    Returns list of full file paths.\n",
    "    \"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the main window\n",
    "    \n",
    "    # Open file dialog for multiple images\n",
    "    file_paths = filedialog.askopenfilenames(\n",
    "        title=\"Select images to predict\",\n",
    "        filetypes=[\n",
    "            (\"Image files\", \"*.jpg *.jpeg *.png *.bmp *.tiff *.gif\"),\n",
    "            (\"All files\", \"*.*\")\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    root.destroy()\n",
    "    return list(file_paths)\n",
    "\n",
    "def visualize_predictions_from_files(image_paths: list, class_names: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Visualize predictions from a list of image file paths (from tkinter).\n",
    "    \"\"\"\n",
    "    if not image_paths:\n",
    "        print(\"No images selected.\")\n",
    "        return\n",
    "    \n",
    "    rows = math.ceil(len(image_paths) / COLUMNS)\n",
    "    plt.figure(figsize=(COLUMNS * 4, rows * 4))\n",
    "    \n",
    "    for i, img_path in enumerate(image_paths):\n",
    "        # Load image\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"‚ö†Ô∏è Could not load {img_path}\")\n",
    "            continue\n",
    "            \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Preprocess\n",
    "        img_resized = cv2.resize(img, (75, 75))\n",
    "        img_norm = img_resized / 255.0\n",
    "        img_input = np.expand_dims(img_norm, axis=0)\n",
    "        \n",
    "        # Predict\n",
    "        preds = model.predict(img_input, verbose=0)\n",
    "        class_id = np.argmax(preds[0])\n",
    "        confidence = preds[0][class_id]\n",
    "        filename = os.path.basename(img_path)  # Just filename for title\n",
    "        label = f\"{class_names[class_id]} ({confidence:.2f})\"\n",
    "        \n",
    "        # Plot\n",
    "        plt.subplot(rows, COLUMNS, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"{filename[:20]}...\\n{label}\", fontsize=10)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage - replaces the old line\n",
    "class_names = ['Finished', 'Opened', 'Sealed']  # Update this for cup noodles!\n",
    "image_files = select_images_with_tkinter()\n",
    "visualize_predictions_from_files(image_files, class_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
