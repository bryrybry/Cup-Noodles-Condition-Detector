{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ff643c",
   "metadata": {},
   "source": [
    "# 1. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a27776",
   "metadata": {},
   "source": [
    "### ‚öñÔ∏è Check Class Balance\n",
    "**Displays the number and percentage of images per class.**\n",
    "\n",
    "*(Ensuring that each class has a similar amount of images will improve the model)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9f1fd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts: {'Finished': 438, 'Opened': 287, 'Sealed': 314}\n",
      "Ratios: {'Finished': '42.2%', 'Opened': '27.6%', 'Sealed': '30.2%'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def check_balance(data_dir='./data'):\n",
    "    counts = {}\n",
    "    folders = []  \n",
    "    \n",
    "    for class_dir in os.listdir(data_dir):\n",
    "        class_path = os.path.join(data_dir, class_dir)\n",
    "        if os.path.isdir(class_path):\n",
    "            folders.append(class_dir)\n",
    "            counts[class_dir] = len(os.listdir(class_path))\n",
    "    \n",
    "    print(\"Counts:\", counts)\n",
    "    print(\"Ratios:\", {k: f\"{(counts[k]/sum(counts.values())*100):.1f}%\" for k in folders})\n",
    "\n",
    "check_balance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f0b6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking dataset...\n",
      "Dataset Balance:\n",
      "{'Finished': 438, 'Opened': 287, 'Sealed': 314}\n",
      "Ratios: {'Finished': '42.2%', 'Opened': '27.6%', 'Sealed': '30.2%'}\n",
      "\n",
      "üìä Loading data...\n",
      "Found 833 images belonging to 3 classes.\n",
      "Found 911 images belonging to 3 classes.\n",
      "Found 128 images belonging to 3 classes.\n",
      "\n",
      "üèóÔ∏è Building model...\n",
      "\n",
      "‚öñÔ∏è Computing class weights...\n",
      "Class weights: {0: np.float64(0.7910731244064577), 1: np.float64(1.2072463768115942), 2: np.float64(1.1018518518518519)}\n",
      "\n",
      "üöÄ Training...\n",
      "Epoch 1/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5876 - loss: 1.1650"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 124\u001b[39m\n\u001b[32m    118\u001b[39m callbacks = [\n\u001b[32m    119\u001b[39m     EarlyStopping(patience=\u001b[32m5\u001b[39m, restore_best_weights=\u001b[38;5;28;01mTrue\u001b[39;00m, monitor=\u001b[33m'\u001b[39m\u001b[33mval_accuracy\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m    120\u001b[39m     ReduceLROnPlateau(factor=\u001b[32m0.2\u001b[39m, patience=\u001b[32m3\u001b[39m, min_lr=\u001b[32m1e-7\u001b[39m)\n\u001b[32m    121\u001b[39m ]\n\u001b[32m    123\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müöÄ Training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mEPOCHS\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weight_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Handles your imbalance!\u001b[39;49;00m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    131\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ Test accuracy:\u001b[39m\u001b[33m\"\u001b[39m, model.evaluate(test_gen)[\u001b[32m1\u001b[39m])\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# Save with timestamp\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bryon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bryon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:413\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validation_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_eval(\n\u001b[32m    409\u001b[39m     epoch, validation_freq\n\u001b[32m    410\u001b[39m ):\n\u001b[32m    411\u001b[39m     \u001b[38;5;66;03m# Create EpochIterator for evaluation and cache it.\u001b[39;00m\n\u001b[32m    412\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_eval_epoch_iterator\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m         \u001b[38;5;28mself\u001b[39m._eval_epoch_iterator = \u001b[43mTFEpochIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m            \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m            \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m            \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m            \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    423\u001b[39m     val_logs = \u001b[38;5;28mself\u001b[39m.evaluate(\n\u001b[32m    424\u001b[39m         x=val_x,\n\u001b[32m    425\u001b[39m         y=val_y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    431\u001b[39m         _use_cached_eval_dataset=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    432\u001b[39m     )\n\u001b[32m    433\u001b[39m     val_logs = {\n\u001b[32m    434\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mval_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs.items()\n\u001b[32m    435\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bryon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:750\u001b[39m, in \u001b[36mTFEpochIterator.__init__\u001b[39m\u001b[34m(self, distribute_strategy, *args, **kwargs)\u001b[39m\n\u001b[32m    748\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(*args, **kwargs)\n\u001b[32m    749\u001b[39m \u001b[38;5;28mself\u001b[39m._distribute_strategy = distribute_strategy\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m dataset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_adapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf.distribute.DistributedDataset):\n\u001b[32m    752\u001b[39m     dataset = \u001b[38;5;28mself\u001b[39m._distribute_strategy.experimental_distribute_dataset(\n\u001b[32m    753\u001b[39m         dataset\n\u001b[32m    754\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bryon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:307\u001b[39m, in \u001b[36mPyDatasetAdapter.get_tf_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_batches \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    305\u001b[39m     num_samples = \u001b[38;5;28mmin\u001b[39m(num_samples, num_batches)\n\u001b[32m    306\u001b[39m batches = [\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28mself\u001b[39m._standardize_batch(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpy_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m    308\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)\n\u001b[32m    309\u001b[39m ]\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batches) == \u001b[32m0\u001b[39m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe PyDataset has length 0\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bryon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:71\u001b[39m, in \u001b[36mIterator.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     67\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_index_array()\n\u001b[32m     68\u001b[39m index_array = \u001b[38;5;28mself\u001b[39m.index_array[\n\u001b[32m     69\u001b[39m     \u001b[38;5;28mself\u001b[39m.batch_size * idx : \u001b[38;5;28mself\u001b[39m.batch_size * (idx + \u001b[32m1\u001b[39m)\n\u001b[32m     70\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_batches_of_transformed_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_array\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bryon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:316\u001b[39m, in \u001b[36mBatchFromFilesMixin._get_batches_of_transformed_samples\u001b[39m\u001b[34m(self, index_array)\u001b[39m\n\u001b[32m    314\u001b[39m filepaths = \u001b[38;5;28mself\u001b[39m.filepaths\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(index_array):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     img = \u001b[43mimage_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilepaths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     x = image_utils.img_to_array(img, data_format=\u001b[38;5;28mself\u001b[39m.data_format)\n\u001b[32m    324\u001b[39m     \u001b[38;5;66;03m# Pillow images should be closed after `load_img`,\u001b[39;00m\n\u001b[32m    325\u001b[39m     \u001b[38;5;66;03m# but not PIL images.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bryon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\image_utils.py:304\u001b[39m, in \u001b[36mload_img\u001b[39m\u001b[34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[39m\n\u001b[32m    302\u001b[39m             img = img.resize(width_height_tuple, resample, box=crop_box)\n\u001b[32m    303\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m             img = \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidth_height_tuple\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bryon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\PIL\\Image.py:2303\u001b[39m, in \u001b[36mImage.resize\u001b[39m\u001b[34m(self, size, resample, box, reducing_gap)\u001b[39m\n\u001b[32m   2300\u001b[39m     im = im.resize(size, resample, box)\n\u001b[32m   2301\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m im.convert(\u001b[38;5;28mself\u001b[39m.mode)\n\u001b[32m-> \u001b[39m\u001b[32m2303\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2305\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reducing_gap \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m resample != Resampling.NEAREST:\n\u001b[32m   2306\u001b[39m     factor_x = \u001b[38;5;28mint\u001b[39m((box[\u001b[32m2\u001b[39m] - box[\u001b[32m0\u001b[39m]) / size[\u001b[32m0\u001b[39m] / reducing_gap) \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bryon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\PIL\\ImageFile.py:406\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    403\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[32m    405\u001b[39m b = b + s\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m n, err_code = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m0\u001b[39m:\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "CONFIG = {\n",
    "    'IMG_SIZE': (75, 75),\n",
    "    'BATCH_SIZE': 32, \n",
    "    'EPOCHS': 20,\n",
    "    'NUM_CLASSES': 3,\n",
    "    'DATA_DIR': './data'\n",
    "}\n",
    "\n",
    "for config in CONFIG:\n",
    "    print(config)\n",
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c4908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_names = ['Finished', 'Opened', 'Sealed']\n",
    "\n",
    "def check_dataset_balance(data_dir):\n",
    "    \"\"\"Verify class distribution before training.\"\"\"\n",
    "    counts = {}\n",
    "    for class_dir in sorted(os.listdir(data_dir)):  # Consistent order\n",
    "        class_path = os.path.join(data_dir, class_dir)\n",
    "        if os.path.isdir(class_path):\n",
    "            counts[class_dir] = len([f for f in os.listdir(class_path) \n",
    "                                   if f.lower().endswith(('.jpg', '.png', '.jpeg'))])\n",
    "    \n",
    "    total = sum(counts.values())\n",
    "    print(\"Dataset Balance:\")\n",
    "    print(counts)\n",
    "    print(\"Ratios:\", {k: f\"{v/total*100:.1f}%\" for k,v in counts.items()})\n",
    "    return counts\n",
    "\n",
    "def get_image_generators(data_dir, img_size, batch_size):\n",
    "    \"\"\"Simplified 80/10/10 split - more reliable than validation_split trick.\"\"\"\n",
    "    seed = 42\n",
    "    \n",
    "    # Train: augmentation\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        validation_split=0.2  # 80/20 split\n",
    "    )\n",
    "    \n",
    "    train_gen = train_datagen.flow_from_directory(\n",
    "        data_dir, target_size=img_size, batch_size=batch_size,\n",
    "        class_mode='categorical', subset='training', seed=seed\n",
    "    )\n",
    "    \n",
    "    # Val/Test: no augmentation\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.125)  # 10% val, 10% test\n",
    "    \n",
    "    val_gen = val_datagen.flow_from_directory(\n",
    "        data_dir, target_size=img_size, batch_size=batch_size,\n",
    "        class_mode='categorical', subset='training', seed=seed, shuffle=False\n",
    "    )\n",
    "    \n",
    "    test_gen = val_datagen.flow_from_directory(\n",
    "        data_dir, target_size=img_size, batch_size=batch_size,\n",
    "        class_mode='categorical', subset='validation', seed=seed, shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_gen, val_gen, test_gen\n",
    "\n",
    "def build_improved_model(img_shape, num_classes):\n",
    "    \"\"\"Better architecture: GlobalAvgPool2D + more layers.\"\"\"\n",
    "    base_model = InceptionV3(input_shape=(*img_shape, 3), include_top=False, weights='imagenet')\n",
    "    \n",
    "    # Freeze base\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Better head (no Flatten = less params)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "# MAIN TRAINING PIPELINE\n",
    "print(\"üîç Checking dataset...\")\n",
    "check_dataset_balance(CONFIG['DATA_DIR'])\n",
    "\n",
    "print(\"\\nüìä Loading data...\")\n",
    "train_gen, val_gen, test_gen = get_image_generators(\n",
    "    CONFIG['DATA_DIR'], CONFIG['IMG_SIZE'], CONFIG['BATCH_SIZE']\n",
    ")\n",
    "\n",
    "print(\"\\nüèóÔ∏è Building model...\")\n",
    "model = build_improved_model(CONFIG['IMG_SIZE'], CONFIG['NUM_CLASSES'])\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "# model.summary()\n",
    "\n",
    "# Compute class weights for imbalance (52%/35%/13% ‚Üí weights adjust)\n",
    "print(\"\\n‚öñÔ∏è Computing class weights...\")\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced', classes=np.unique(train_gen.classes), y=train_gen.classes\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(\"Class weights:\", class_weight_dict)\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, restore_best_weights=True, monitor='val_accuracy'),\n",
    "    ReduceLROnPlateau(factor=0.2, patience=3, min_lr=1e-7)\n",
    "]\n",
    "\n",
    "print(\"\\nüöÄ Training...\")\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=CONFIG['EPOCHS'],\n",
    "    validation_data=val_gen,\n",
    "    class_weight=class_weight_dict,  # Handles your imbalance!\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Test accuracy:\", model.evaluate(test_gen)[1])\n",
    "\n",
    "# Save with timestamp\n",
    "import datetime\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model.save(f'./models/cup_noodle_classifier_{timestamp}.keras')\n",
    "print(f\"üíæ Saved: cup_noodle_classifier_{timestamp}.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6963c87",
   "metadata": {},
   "source": [
    "### ‚≠ê Save the Model as a .keras File (so it can be reused later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd5e50db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if not model: \n",
    "    print(\"There's no model yet!\")\n",
    "    sys.exit()  \n",
    "\n",
    "# get id for new model\n",
    "folder = \"./models/\"\n",
    "id = len([f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))])\n",
    "default_filename = f\"model{id}\"\n",
    "\n",
    "new_model_filename = input(f\"Enter filename (Default: {default_filename}): \")\n",
    "if new_model_filename.strip() == \"\":\n",
    "    new_model_filename = default_filename\n",
    "tf.keras.models.save_model(model, f'./models/{new_model_filename}.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cb305a",
   "metadata": {},
   "source": [
    "# 2. Trying the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f7e9e",
   "metadata": {},
   "source": [
    "### üîÉ Load a .keras model\n",
    "Loading a model is necessary if the above code to train a new model was not executed.\n",
    "(Skip this if model was recently trained above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a8dfae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model Loaded: cup_noodle_classifier_20260128_021448.keras\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "folder = \"./models/\"\n",
    "files = os.listdir(folder)\n",
    "\n",
    "# sort by last added time\n",
    "if not files: raise FileNotFoundError(\"No models found!\")\n",
    "files_sorted = sorted(files, key=lambda f: os.path.getmtime(os.path.join(folder, f)))\n",
    "last_added_filename = files_sorted[-1]\n",
    "\n",
    "while True:\n",
    "    input_filename = input(\n",
    "        f\"Enter model to load (Latest: {last_added_filename.replace('.keras', '')}): \"\n",
    "    ).strip()\n",
    "\n",
    "    if input_filename == \"\":\n",
    "        model_filename = last_added_filename\n",
    "        break\n",
    "\n",
    "    model_filename = f\"{input_filename.replace('.keras', '')}.keras\"\n",
    "    full_path = os.path.join(folder, model_filename)\n",
    "\n",
    "    if os.path.exists(full_path):\n",
    "        break\n",
    "    else:\n",
    "        print(\"‚ùå Model not found. Please try again.\")\n",
    "\n",
    "model = tf.keras.models.load_model(f'./models/{model_filename}')\n",
    "\n",
    "print(\"‚úÖ Model Loaded:\", model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c72f5ac",
   "metadata": {},
   "source": [
    "### Part 2A: Inference using Tensorflow Camera\n",
    "(Please view the cell under this one to terminate the camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6a6ca96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera Started!\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import tensorflow as tf\n",
    "\n",
    "def import_and_predict(image_data):\n",
    "    image = ImageOps.fit(image_data, (75, 75), Image.Resampling.LANCZOS)\n",
    "    image = image.convert('RGB')\n",
    "    image = np.asarray(image)\n",
    "    image = (image.astype(np.float32) / 255.0)\n",
    "    img_reshape = image[np.newaxis,...]\n",
    "    prediction = model.predict(img_reshape, verbose=0)\n",
    "    return prediction\n",
    "\n",
    "stop_flag = False\n",
    "\n",
    "def camera_loop():\n",
    "    global stop_flag\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        cap.open()\n",
    "        \n",
    "    print(\"Camera Started!\")\n",
    "\n",
    "    while not stop_flag:\n",
    "        ret, original = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image = Image.fromarray(cv2.cvtColor(original, cv2.COLOR_BGR2RGB))\n",
    "        prediction = import_and_predict(image)\n",
    "\n",
    "        map = {0: \"Finished\",\n",
    "               1: \"Opened\",\n",
    "               2: \"Sealed\"}\n",
    "\n",
    "        idx = np.argmax(prediction)\n",
    "        confidence = prediction[0][idx] \n",
    "\n",
    "        predict = f\"It is a {map[idx]}! ({confidence:.2f})\"\n",
    "\n",
    "        cv2.putText(original, predict, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "        cv2.imshow(\"Cup Noodles Detector\", original)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            stop_flag = True\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# start the camera in a separate thread\n",
    "camera_thread = threading.Thread(target=camera_loop)\n",
    "camera_thread.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0569d8e4",
   "metadata": {},
   "source": [
    "‚úã Run to terminate the camera!\n",
    "   üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c4efcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped!\n"
     ]
    }
   ],
   "source": [
    "stop_flag = True\n",
    "camera_thread.join()\n",
    "print(\"Stopped!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f930f5cb",
   "metadata": {},
   "source": [
    "### Part 2B: Inference using Folder of Images\n",
    "For this part, we will classify your images based on uploaded images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf3257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "\n",
    "COLUMNS = 5\n",
    "\n",
    "def select_images_with_tkinter():\n",
    "    \"\"\"\n",
    "    Opens tkinter file dialog to select multiple image files.\n",
    "    Returns list of full file paths.\n",
    "    \"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the main window\n",
    "    \n",
    "    # Open file dialog for multiple images\n",
    "    file_paths = filedialog.askopenfilenames(\n",
    "        title=\"Select images to predict\",\n",
    "        filetypes=[\n",
    "            (\"Image files\", \"*.jpg *.jpeg *.png *.bmp *.tiff *.gif\"),\n",
    "            (\"All files\", \"*.*\")\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    root.destroy()\n",
    "    return list(file_paths)\n",
    "\n",
    "def visualize_predictions_from_files(image_paths: list, class_names: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Visualize predictions from a list of image file paths (from tkinter).\n",
    "    \"\"\"\n",
    "    if not image_paths:\n",
    "        print(\"No images selected.\")\n",
    "        return\n",
    "    \n",
    "    rows = math.ceil(len(image_paths) / COLUMNS)\n",
    "    plt.figure(figsize=(COLUMNS * 4, rows * 4))\n",
    "    \n",
    "    for i, img_path in enumerate(image_paths):\n",
    "        # Load image\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"‚ö†Ô∏è Could not load {img_path}\")\n",
    "            continue\n",
    "            \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Preprocess\n",
    "        img_resized = cv2.resize(img, (75, 75))\n",
    "        img_norm = img_resized / 255.0\n",
    "        img_input = np.expand_dims(img_norm, axis=0)\n",
    "        \n",
    "        # Predict\n",
    "        preds = model.predict(img_input, verbose=0)\n",
    "        class_id = np.argmax(preds[0])\n",
    "        confidence = preds[0][class_id]\n",
    "        filename = os.path.basename(img_path)  # Just filename for title\n",
    "        label = f\"{class_names[class_id]} ({confidence:.2f})\"\n",
    "        \n",
    "        # Plot\n",
    "        plt.subplot(rows, COLUMNS, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"{filename[:20]}...\\n{label}\", fontsize=10)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage - replaces the old line\n",
    "class_names = ['Finished', 'Opened', 'Sealed']  # Update this for cup noodles!\n",
    "image_files = select_images_with_tkinter()\n",
    "visualize_predictions_from_files(image_files, class_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
